{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 05 - TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPBKAjhXizpz"
      },
      "source": [
        "# Lab 05 - TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzstop1YjRVW"
      },
      "source": [
        "#Release: 1.2107.2401"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPkaisw4jTme"
      },
      "source": [
        "You will learn how to:\n",
        "1. Calculate TF-IDF using TfidfVectorizer\n",
        "2. View data in pandas DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwdpkiZbjZX5"
      },
      "source": [
        "<br>\n",
        " \n",
        "***If you use Google Colab, install sastrawi package***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk4S5JbHLsn-"
      },
      "source": [
        "!pip install sastrawi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bwF4tOYje9K"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### Import required library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHwJ64XALuaC"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6kwi2REjmf0"
      },
      "source": [
        "<br>\n",
        " \n",
        "***If you use Google Colab, download stopwords dan punkt package***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFB9CxjkLx0E"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOIcHPlQcP18"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('indonesian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSOT8Gg1cR5d"
      },
      "source": [
        "len(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFO-W1AcUv4"
      },
      "source": [
        "stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QoiUZBTjsI2"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### Prepocessing function from previous labs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoOt506bcKfI"
      },
      "source": [
        "def tokenize_clean(text):\n",
        "    \n",
        "    #tokenisasi\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word\n",
        "        in nltk.word_tokenize(sent)]\n",
        "    \n",
        "    #clean token from numeric and other character like puntuation\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    return filtered_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk1L7qijcLU-"
      },
      "source": [
        "def remove_stopwords(tokenized_text):\n",
        "    \n",
        "    cleaned_token = []\n",
        "    for token in tokenized_text:\n",
        "        if token not in stopwords:\n",
        "            cleaned_token.append(token)\n",
        "            \n",
        "    return cleaned_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4hm645wcdza"
      },
      "source": [
        "def stemming_text(tokenized_text):\n",
        "    \n",
        "    #stem using Sastrawi StemmerFactory \n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "\n",
        "    stems = []\n",
        "    for token in tokenized_text:\n",
        "        stems.append(stemmer.stem(token))\n",
        "\n",
        "    return stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_As_alpOcgAI"
      },
      "source": [
        "def text_preprocessing(text):\n",
        "    \n",
        "    prep01 = tokenize_clean(text)\n",
        "    prep02 = remove_stopwords(prep01)\n",
        "    prep03 = stemming_text(prep02)\n",
        "    \n",
        "    return prep03"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fj8twfYkVkG"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Small Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nadSWv96kH8I"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Step 01 - Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxiaQ41pMMCY"
      },
      "source": [
        "dataset = [\n",
        "     'kucing kucing kucing hitam putih belang',\n",
        "     'tikus belang',\n",
        "     'tikus hitam',\n",
        "     'tikus tikus tikus'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh0wigJAMRbB"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fez0izY9Mczj"
      },
      "source": [
        "dataset[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoquyArRkfJK"
      },
      "source": [
        "### Step 02 - Compute TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esNvMZ-iMfDI"
      },
      "source": [
        "#perform tf-idf vectorization\n",
        "vectorizer = TfidfVectorizer(use_idf=True)\n",
        "result_tfidf = vectorizer.fit_transform(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E32QIDCrKKf"
      },
      "source": [
        "### Step 03 - View Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CdNRW1ik5x4"
      },
      "source": [
        "Get List of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtZlyIISctuz"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak8WthuufRXU"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5jiF5UClCLg"
      },
      "source": [
        "View TF-IDF Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRVEE7sGc32K"
      },
      "source": [
        "print(result_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-EdK5bBrI6h"
      },
      "source": [
        "type(result_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqk5zmUAeM1s"
      },
      "source": [
        "print(result_tfidf.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX7jaNoylH2k"
      },
      "source": [
        "View First Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ePTMu4ceibT"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in2d_r6heXsH"
      },
      "source": [
        "print(result_tfidf[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GB01zE0lPPb"
      },
      "source": [
        "print(result_tfidf[0].toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dt6_5O-lYfV"
      },
      "source": [
        "View Second Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl8w_Nr9d2vy"
      },
      "source": [
        "print(result_tfidf[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aqKlPERr25A"
      },
      "source": [
        "print(result_tfidf[1].toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEHQPomLsPjn"
      },
      "source": [
        "dataset[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjKXHr2YsLDh"
      },
      "source": [
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imUIFDIIf63q"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(result_tfidf[1].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
        "df.sort_values(by=['TF-IDF'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY3QYt29lvfU"
      },
      "source": [
        "<br>\n",
        "\n",
        "View IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTdLEXiTgUQ1"
      },
      "source": [
        "# print idf values\n",
        "df_idf = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names(),columns=[\"idf\"])\n",
        " \n",
        "# sort ascending\n",
        "df_idf.sort_values(by=['idf'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XyAFiWyl_sy"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Step 04 - Compute TF-IDF with new sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4dPj8j0iYUS"
      },
      "source": [
        "new_text = 'kambing hitam'\n",
        "result_tfidf = vectorizer.transform([new_text])\n",
        "\n",
        "feature_names = vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pT-vfHricyC"
      },
      "source": [
        "feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10mDluHHid0Y"
      },
      "source": [
        "result_tfidf.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9NPK_ChinSB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grl6uBNNmT9h"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Bigger Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EHA6QH9pc2v"
      },
      "source": [
        "### Step 01 - Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X21I0UT4piu6"
      },
      "source": [
        "files = []\n",
        "files.append(\"Sekelompok ibu dan kaum perempuan duduk beralaskan rumput lapangan sambil fokus menganyam bambu yang ia genggam ditangan.\")\n",
        "files.append(\"Sebagian besar masyarakat rupanya tak mau melewatkan waktu begitu  saja untuk meratapi erupsi.\")\n",
        "files.append(\"Lombok memang memiliki sejuta pesona yang mampu menyedot perhatian orang untuk datang berwisata.\")\n",
        "files.append(\"Perempuan yang bergelut di dunia kerelawanan akan belajar caranya bertanggung jawab bagi sendiri dan orang lain.\")\n",
        "files.append(\"Kami berkoordinasi dan melapor pada posko relawan, kami berkomitmen  siap membantu dengan siaga 24 jam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRXrMUKqppuw"
      },
      "source": [
        "### Step 02 - Corpus preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1VOo-3EpohN"
      },
      "source": [
        "token_dict = {}\n",
        "i = 0\n",
        "for t in files:\n",
        "    filename = \"file\" + str(i)\n",
        "    token_dict[filename] = t\n",
        "    i = i + 1\n",
        "\n",
        "token_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JanUdBjqp0Pw"
      },
      "source": [
        "token_dict.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6uLb1ugqGcj"
      },
      "source": [
        "token_dict['file0']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY_Dth7ZqRk9"
      },
      "source": [
        "### Step 03 - Compute TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjGBAEsXqVJH"
      },
      "source": [
        "#perform tf-idf vectorization\n",
        "tfidf = TfidfVectorizer(max_df=0.8,             # terms with document frequency value > 0.8 will be removed\n",
        "                        min_df=0.2,             # terms with document frequency value < 0.2 will be removed\n",
        "                        max_features=200000,    # create maximum 200.000 vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
        "                        stop_words = stopwords, # stopwords list\n",
        "                        use_idf=True,           # enable inverse-document-frequency reweighting\n",
        "                        tokenizer=text_preprocessing, # override the string tokenization step by using text_prepocessing function \n",
        "                        ngram_range=(1,3))      # ngram range 1 - 3 \n",
        "\n",
        "\n",
        "tfs = tfidf.fit_transform(token_dict.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQh8dz0Uql6I"
      },
      "source": [
        "For detail TfidfVectorizer documentation visit: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qoWKH9IrCt4"
      },
      "source": [
        "### Step 04 - View Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N784unptrXLt"
      },
      "source": [
        "Let's check the shape. We should have 5 rows (5 docs) and 96 columns (96 unique words):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wqe_LGLrbjq"
      },
      "source": [
        "tfs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrOAkbdnrhZa"
      },
      "source": [
        "<br>\n",
        "\n",
        "Inspect the first document vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfS6NR9irlik"
      },
      "source": [
        "print(tfs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYb4Z4-urwaF"
      },
      "source": [
        "View the list of feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU2crnuirw0y"
      },
      "source": [
        "feature_names = tfidf.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enA-rLTFr8cZ"
      },
      "source": [
        "print(len(feature_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvvVMDfyr-Q6"
      },
      "source": [
        "print(feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McHFdZQisPdO"
      },
      "source": [
        "# print idf values\n",
        "df_idf = pd.DataFrame(tfidf.idf_, index=feature_names,columns=[\"idf\"])\n",
        " \n",
        "# sort ascending\n",
        "df_idf.sort_values(by=['idf'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTY9PBaesT_G"
      },
      "source": [
        "### Step 04 - New sentence TF-IDF transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b80QEVsosXaH"
      },
      "source": [
        "str1 = 'Di kejauhan tampak seorang relawan pria dari Lombok sedang berjalan.'\n",
        "response = tfidf.transform([str1])\n",
        "\n",
        "#show result\n",
        "for col in response.nonzero()[1]:\n",
        "    print (feature_names[col], ' - ', response[0, col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T--0lLGgslsY"
      },
      "source": [
        "print(response[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V3FvTI_s-Y2"
      },
      "source": [
        "print (text_preprocessing(str1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysrM0z_HtHUd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIl9a2QYi_kr"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "#### Revision History:\n",
        "Release: 1.1907.1601\n",
        "- Initial release\n",
        "\n",
        "Release: 1.1909.0901\n",
        "- Install sastrawi package to support Google Colab\n",
        "- Reorganize code\n",
        "\n",
        "Release: 1.2011.2701\n",
        "- Create small sample to make easier to understand\n",
        "\n",
        "Release: 1.2107.2401\n",
        "- Tidyup the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXlWSNYjjANZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}