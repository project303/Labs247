{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "colab": {
      "name": "Lab_Extra_02_Sentiment_Analysis_Id.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfUnoZ8_Kw_o"
      },
      "source": [
        "# SENTIMENT ANALYSIS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhGLLyYIKw_2"
      },
      "source": [
        "#Release: 1.2102.0601"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYTx-IJYKxAR"
      },
      "source": [
        "# Library\n",
        "\n",
        "For this lab, we will need ``wordcloud`` library.\n",
        "Use pip to install the library from Anaconda prompt : ``pip install wordcloud``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bvihJgEKxAX"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import SklearnClassifier\n",
        "\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hPLlLJqKxAu"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPCicySSTqoG"
      },
      "source": [
        "!mkdir -p dataset\n",
        "!wget https://raw.githubusercontent.com/project303/dataset/master/Twitter.csv -P dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtVwU5ajTw1L"
      },
      "source": [
        "!ls dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haE4E1ri2q0b"
      },
      "source": [
        "Get number of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Uz_TQ0KxA0"
      },
      "source": [
        "data = pd.read_csv('dataset/Twitter.csv', sep='|')\n",
        "# Choose the column we will be using\n",
        "data = data[['text','sentiment']]\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNa9mXI720WC"
      },
      "source": [
        "Sample of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu2KSshuKxBJ"
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4acQwfk225_g"
      },
      "source": [
        "data.groupby(\"sentiment\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NqvdY243ZWA"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\r\n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UsU2lpG4Kdh"
      },
      "source": [
        "data[(data.sentiment == 'Negatif')].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE1ObOw043Ll"
      },
      "source": [
        "Split Data into Training and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grlf4XI4KxBb"
      },
      "source": [
        "# Bagi dataset menjadi data training dan testing\n",
        "train, test = train_test_split(data,test_size = 0.2)\n",
        "\n",
        "# Hapus sentiment yang netral\n",
        "train = train[train.sentiment != \"Netral\"]\n",
        "\n",
        "# Pisahkan data trining positif dan negatif\n",
        "train_pos = train[ train['sentiment'] == 'Positif']\n",
        "train_pos = train_pos['text']\n",
        "train_neg = train[ train['sentiment'] == 'Negatif']\n",
        "train_neg = train_neg['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-3aU33Q8sTJ"
      },
      "source": [
        "train.groupby(\"sentiment\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z36FkkNiKxBp"
      },
      "source": [
        "train_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bdkZE1aKxB3"
      },
      "source": [
        "train_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur60olHN5l3c"
      },
      "source": [
        "Draw WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSxXa_mxKxDF"
      },
      "source": [
        "def wordcloud_draw(data, color = 'black'):\n",
        "    words = ' '.join(data)\n",
        "    cleaned_word = \" \".join([word for word in words.split()\n",
        "                            if 'http' not in word\n",
        "                                and not word.startswith('@')\n",
        "                                and not word.startswith('#')\n",
        "                                and word != 'RT'\n",
        "                            ])\n",
        "    wordcloud = WordCloud(stopwords=stopwords_all,\n",
        "                      background_color=color,\n",
        "                      width=2500,\n",
        "                      height=2000\n",
        "                     ).generate(cleaned_word)\n",
        "    plt.figure(1,figsize=(13, 13))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Positive words\")\n",
        "wordcloud_draw(train_pos,'white')\n",
        "print(\"Negative words\")\n",
        "wordcloud_draw(train_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_4CCysbKxCi"
      },
      "source": [
        "Add Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJe94w56KxCn"
      },
      "source": [
        "#augment the stopwords with nonstandard twitter words\n",
        "stopwords_set = set(stopwords.words(\"indonesian\"))\n",
        "stopwords_aug = {\"ya\",\"yak\",\"iya\",\"yg\",\"ga\",\"gak\",\"gk\",\"udh\",\"sdh\",\"udah\",\"dah\",\"nih\",\"ini\",\"deh\",\"sih\",\"dong\",\"donk\",\n",
        "                 \"sm\",\"knp\",\"utk\",\"yaa\",\"tdk\",\"gini\",\"gitu\",\"bgt\",\"gt\",\"nya\",\"kalo\",\"cb\",\"jg\",\"jgn\",\"gw\",\"ge\",\n",
        "                 \"sy\",\"min\",\"mas\",\"mba\",\"mbak\",\"pak\",\"kak\",\"trus\",\"trs\",\"bs\",\"bisa\",\"aja\",\"saja\",\"no\",\n",
        "                 \"w\",\"g\",\"gua\",\"gue\",\"emang\",\"emg\",\"wkwk\",\"dr\",\"kau\",\"dg\",\"gimana\",\"apapun\",\"apa\",\n",
        "                 \"klo\",\"yah\",\"banget\",\"pake\",\"terus\",\"krn\",\"jadi\",\"jd\",\"mu\",\"ku\",\"si\",\"hehe\",\n",
        "                 \"tp\",\"pa\",\"lu\",\"lo\",\"lw\",\"tw\",\"tau\",\"karna\",\"kayak\",\"ky\",\"lg\",\"untuk\",\"tuk\",\"dg\",\"dgn\"}\n",
        "stopwords_all = stopwords_set.union(stopwords_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4uDfBGqKxC4"
      },
      "source": [
        "#stopwords_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8noMtF06qYb"
      },
      "source": [
        "Data Preparation for Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acEv2kePKxDj"
      },
      "source": [
        "# tweets adalah data training yang telah dibersihkan\n",
        "tweets = []\n",
        "#stopwords_set = set(stopwords.words(\"indonesia\"))\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n",
        "    words_cleaned = [word for word in words_filtered\n",
        "        if 'http' not in word\n",
        "        and not word.startswith('@')\n",
        "        and not word.startswith('#')\n",
        "        and word != 'RT']\n",
        "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_all]\n",
        "    tweets.append((words_cleaned,row.sentiment))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSbn_d_Z68WL"
      },
      "source": [
        "tweets[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMZbyrt-WgBN"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KVxi9L_9NKE"
      },
      "source": [
        "Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb-rQQpKKxEQ"
      },
      "source": [
        "# Extracting word features\n",
        "def get_words_in_tweets(tweets):\n",
        "    all = []\n",
        "    for (words, sentiment) in tweets:\n",
        "        all.extend(words)\n",
        "    return all\n",
        "\n",
        "def get_word_features(wordlist):\n",
        "    wordlist = nltk.FreqDist(wordlist)\n",
        "    features = wordlist.keys()\n",
        "    return features\n",
        "w_features = get_word_features(get_words_in_tweets(tweets))\n",
        "\n",
        "\n",
        "def extract_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in w_features:\n",
        "        features['containts(%s)' % word] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "\n",
        "wordcloud_draw(w_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA7u0NBC-7iu"
      },
      "source": [
        "Sentiment Classification Using Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtCbw3ZU9IZ0"
      },
      "source": [
        "# Training the Naive Bayes classifier\r\n",
        "training_set = nltk.classify.apply_features(extract_features,tweets)\r\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75-tHyLA911G"
      },
      "source": [
        "print(\"Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, training_set))*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy8Q3f88-J-S"
      },
      "source": [
        "Test Classifier Using Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMsP2GTR6oPi"
      },
      "source": [
        "test_pos = test[ test['sentiment'] == 'Positif']\r\n",
        "test_pos = test_pos['text']\r\n",
        "test_neg = test[ test['sentiment'] == 'Negatif']\r\n",
        "test_neg = test_neg['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_d35y_f94j3"
      },
      "source": [
        "neg_cnt = 0\r\n",
        "pos_cnt = 0\r\n",
        "for obj in test_neg: \r\n",
        "    res =  classifier.classify(extract_features(obj.split()))\r\n",
        "    if(res == 'Negatif'): \r\n",
        "        neg_cnt = neg_cnt + 1\r\n",
        "        #print(obj.split())\r\n",
        "for obj in test_pos: \r\n",
        "    res =  classifier.classify(extract_features(obj.split()))\r\n",
        "    if(res == 'Positif'): \r\n",
        "        pos_cnt = pos_cnt + 1\r\n",
        "        \r\n",
        "print('[Negatif]: %s/%s '  % (len(test_neg),neg_cnt))        \r\n",
        "print('[Positif]: %s/%s '  % (len(test_pos),pos_cnt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLcJxOvAKxDw"
      },
      "source": [
        "test_pos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNT7CaoRKxD_"
      },
      "source": [
        "test_neg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPNRa8I7Wtqp"
      },
      "source": [
        "tweets_test = []\n",
        "#stopwords_set = set(stopwords.words(\"indonesia\"))\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n",
        "    words_cleaned = [word for word in words_filtered\n",
        "        if 'http' not in word\n",
        "        and not word.startswith('@')\n",
        "        and not word.startswith('#')\n",
        "        and word != 'RT']\n",
        "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_all]\n",
        "    tweets_test.append((words_cleaned,row.sentiment))\n",
        "\n",
        "test_set = nltk.classify.apply_features(extract_features,tweets_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrR5-NhjWwio"
      },
      "source": [
        "print(\"Naive Bayes Algo accuracy on test percent:\", (nltk.classify.accuracy(classifier, test_set))*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK6l4URv_Wic"
      },
      "source": [
        "Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUS6Jy9BKxEi"
      },
      "source": [
        "w_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVx4-9HJKxEy"
      },
      "source": [
        "nltk.FreqDist(get_words_in_tweets(tweets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRlfByST_jys"
      },
      "source": [
        "Test Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGcMsw0GXEj8"
      },
      "source": [
        "pred =  classifier.classify(extract_features(['oke', 'nasional', 'banget', 'paket', 'pulsa']))\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1hGVKC_ZPVp"
      },
      "source": [
        "classifier.show_most_informative_features(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RKLRZ2TAf8f"
      },
      "source": [
        "Save Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU8pIFX4KxFZ"
      },
      "source": [
        "#To save the trained claassifier, do the following\n",
        "import pickle\n",
        "f = open('my_classifier.pickle', 'wb')\n",
        "pickle.dump(classifier, f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E83M6QtPKxFp"
      },
      "source": [
        "#To reload it:\n",
        "f = open('my_classifier.pickle', 'rb')\n",
        "classifier = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "137ybphvKxF4"
      },
      "source": [
        "pred = classifier.classify(extract_features(['makasih', 'sinyal', 'banget']))\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkQ-Lx8sWdgS"
      },
      "source": [
        "extract_features(['kementerian', 'sinyal', 'banget'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic-mPO6KA5MQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}